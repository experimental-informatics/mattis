{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Install libraries. '''\n",
    "# !python3 -m pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2534\n",
      "['Über die Form kann inhaltlich Verschiedenes aufeinander bezogen werden, ohne die jeweilige Verschiedenheit aufzugeben.', 'Wenn zwei verschiedene Dinge auf dieselbe formale Struktur bezogen werden können, können sie sich auch aufeinander beziehen.']\n"
     ]
    }
   ],
   "source": [
    "''' Data contains one sentence per line. '''\n",
    "txt = open('dataset.txt', 'r', encoding='utf8').read().split('\\n')\n",
    "print('Number of sentences:', len(txt))\n",
    "print(txt[1000:1002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Tokenize with pretrained word embeddings. '''\n",
    "import spacy\n",
    "# Download language model\n",
    "# !python -m spacy download de_core_news_md\n",
    "# https://spacy.io/models/de#de_core_news_md\n",
    "spacy_de = spacy.load('de_core_news_md')\n",
    "tokenized_txt = [spacy_de(sentence) for sentence in txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = spacy_de('Maschine Algorithmus Software Möglichkeitsraum')\n",
    "s2 = spacy_de('Doktor Fürst Hochgobernitz Gehirn')\n",
    "s3 = spacy_de('Denken Gehirn Fürst')\n",
    "s4 = spacy_de('System Maschine Theorie Implementierung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14237512269583175\n",
      "0.28613596500956817\n",
      "0.8049022142081795 (Should be the highest score.)\n",
      "----------\n",
      "0.14237512269583175\n",
      "0.7948757580953972 (Should be the highest score.)\n",
      "0.24041798065593106\n"
     ]
    }
   ],
   "source": [
    "print(s1.similarity(s2))\n",
    "print(s1.similarity(s3))\n",
    "print(s1.similarity(s4), '(Should be the highest score.)')\n",
    "print('-'*10)\n",
    "print(s2.similarity(s1))\n",
    "print(s2.similarity(s3), '(Should be the highest score.)')\n",
    "print(s2.similarity(s4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute similarities for all sentences. '''\n",
    "def get_similar_sentences(inp, n_items=5):\n",
    "    similarities = []\n",
    "   \n",
    "    for i in range(len(txt)):\n",
    "        # store (index, score)\n",
    "        similarities.append((i, inp.similarity(tokenized_txt[i])))\n",
    "    \n",
    "    # sort by score, return top n_items\n",
    "    similarities_sorted = sorted(similarities, key=lambda item: -item[1])\n",
    "    # (0 is the sentence itself.)\n",
    "    return [[similarities_sorted[i][1], similarities_sorted[i][0], txt[similarities_sorted[i][0]]] for i in range(1,n_items+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je mehr wir uns von derlei Informationsleistungen des Kunstwerks entfernen, je sicherer können wir sein, uns innerhalb der ästhetischen Grenze zu befinden. \n",
      "( 0.8171350114467535 , 354 )\n",
      "\n",
      "Das Selbstkonzept wird umso komplexer, je mehr Dimensionen in dem Bezugsrahmen vorliegen. \n",
      "( 0.7994579607167603 , 1507 )\n",
      "\n",
      "Dadurch können wir beispielsweise erkennen, dass ein Problem sich nicht lösen lässt, auch wenn wir unendlich weiter rechnen. \n",
      "( 0.7958818361169535 , 863 )\n",
      "\n",
      "Vielmehr offeriert sie unseren Gesellschaften ein Potenzial, das wir gestalten können. \n",
      "( 0.7898650538532539 , 475 )\n",
      "\n",
      "Jeder Mensch denkt ursprünglich über das ganze Leben nach, erklärte er aber je genauer er nachdenkt, desto mehr engt sich das ein. \n",
      "( 0.7882662691450477 , 1437 )\n",
      "\n",
      "Zunächst müssen wir uns klar darüber sein, daß das Denken, welches sich als Maschine realisiert und objektiven Status erreichen kann, ein spezifisches Denken ist, das wir operationale Theorie nennen wollen. \n",
      "( 0.7749798229541957 , 1101 )\n",
      "\n",
      "Das Unbewusste ist viel umfassender als das uns Bewusste und lenkt unsere Entscheidungen auch viel mehr als das uns Bewusste. \n",
      "( 0.770994763930887 , 1589 )\n",
      "\n",
      "Das Formale und das Konkrete dürfen aber nicht als Widersacher im Ringen um Fülle und Ästhetik betrachtet werden, sie bilden ein kraftvolles Gespann. \n",
      "( 0.7666233188207121 , 2190 )\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/p/envs/jup/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "sentence = '''\n",
    "Je mehr wir rechnen, desto mehr engt sich das Werden ein.\n",
    "'''.replace('\\n','')\n",
    "sentence = spacy_de(sentence)\n",
    "similar_sentences = get_similar_sentences(sentence, 8)\n",
    "\n",
    "for score, index, sentence in similar_sentences:\n",
    "    print(sentence, '\\n(', score, ',', index, ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/p/envs/jup/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "''' Get similarities for each sentence and store them as one string. '''\n",
    "out = ''\n",
    "# Store the amount of appearances of each sentence in a list.\n",
    "import numpy as np\n",
    "appearances = np.asarray([0] * len(txt))\n",
    "# Loop through dataset.\n",
    "n = len(txt)-1\n",
    "for i in range(n):\n",
    "    # Append input.\n",
    "    out += txt[i] + \"\\n\"\n",
    "    # Append next sentence from original dataset.\n",
    "    out += txt[i+1] + \"\\n\"\n",
    "    similar_sentences = get_similar_sentences(tokenized_txt[i])\n",
    "    for score, index, sentence in similar_sentences:\n",
    "        # Append input and similar sentence\n",
    "        out += txt[i] + \"\\n\"\n",
    "        out += sentence + \"\\n\"\n",
    "        # Increase appearance of that sentence\n",
    "        appearances[index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Write string to disk. '''\n",
    "with open('dataset_augmented.txt', 'w') as f:\n",
    "    f.write(out)\n",
    "    f.flush()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: augment and shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/p/envs/jup/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "''' Create a list of sentence pairs (tuples). '''\n",
    "out = []\n",
    "# Loop through dataset.\n",
    "n = len(txt)-1\n",
    "for i in range(n):\n",
    "    # Append input and following sentence.\n",
    "    out.append((txt[i]+'\\n', txt[i+1]+'\\n'))\n",
    "    # get similar sentences\n",
    "    similar_sentences = get_similar_sentences(tokenized_txt[i])\n",
    "    for score, index, sentence in similar_sentences:\n",
    "        out.append((txt[i]+'\\n', sentence+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_str = ''\n",
    "for pair in out:\n",
    "    out_str += pair[0] + pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Write string to disk. '''\n",
    "with open('dataset_augmented_shuffled.txt', 'w') as f:\n",
    "    f.write(out_str)\n",
    "    f.flush()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "\n",
    "https://medium.com/better-programming/the-beginners-guide-to-similarity-matching-using-spacy-782fc2922f7c\n",
    "\n",
    "https://spacy.io/usage/vectors-similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
