{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with Transformer (TensorFlow Tutorial)\n",
    "\n",
    "Most of this code is provided by The TensorFlow Authors through this tutorial: https://www.tensorflow.org/tutorials/text/transformer\n",
    "\n",
    "Main changes:\n",
    "\n",
    "- Loading a custom dataset\n",
    "- Adjusting parameters\n",
    "- Walking through latent space (last part)\n",
    "- Deleting all code that is not necessary to run the script (explanations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Install libraries. '''\n",
    "# !python3 -m pip install tensorflow-datasets pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load custom dataset through pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt = open('dataset.txt', encoding='utf8').readlines()\n",
    "txt = [sentence.replace('\\n','') for sentence in txt]\n",
    "\n",
    "# Next sentence as target sequence\n",
    "raw_data = {'src': [line for line in txt[:-1]],\n",
    "           'trg': [line for line in txt[1:]]}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=['src', 'trg'])\n",
    "\n",
    "# Split training and validation data\n",
    "df_train, df_val = train_test_split(df, test_size=0.15)\n",
    "\n",
    "# Create dataset objects\n",
    "train_examples = tf.data.Dataset.from_tensor_slices((df_train['src'], df_train['trg']))\n",
    "val_examples = tf.data.Dataset.from_tensor_slices((df_val['src'], df_val['trg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset shape (`next(iter(dataset))`):\n",
    "\n",
    "(<tf.Tensor: shape=(), dtype=string, numpy=b'Paul Val\\xc3\\xa9rys Form: Sie wird gespeist von seinem unerm\\xc3\\xbcdlichen Drang zum Objektivieren und, mit C\\xc3\\xa9zannes Wort, Realisieren, der kein Dunkles, Unaufgehelltes, Ungel\\xc3\\xb6stes duldet; dem die Transparenz nach aussen zum Mass des Gelingens im Innern selbst wird.\\n'>,\n",
    " <tf.Tensor: shape=(), dtype=string, numpy=b'Grosse Einsichten in die Kunst geraten \\xc3\\xbcberhaupt entweder in absoluter Distanz, aus der Konsequenz des Begriffs, ungest\\xc3\\xb6rt vom sogenannten Kunstverst\\xc3\\xa4ndnis, wie bei Kant oder auch Hegel, oder in solcher absoluten N\\xc3\\xa4he, der Haltung dessen, der hinter den Kulissen steht, der nicht Publikum ist, sondern das Kunstwerk mitvollzieht unter dem Aspekt des Machens, der Technik.\\n'>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Run tokenizer. '''\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df['src'], df['trg']))\n",
    "\n",
    "tokenizer_src = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "   (src.numpy() for src, trg in dataset), \n",
    "   target_vocab_size=2**12) #4096\n",
    "tokenizer_src.save_to_file('Seq2Seq/tokenizer_src_training')\n",
    "\n",
    "tokenizer_trg = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "   (trg.numpy() for src, trg in dataset), \n",
    "   target_vocab_size=2**12) #4096\n",
    "tokenizer_trg.save_to_file('Seq2Seq/tokenizer_trg_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load tokenizer. '''\n",
    "\n",
    "# tokenizer_src = tfds.features.text.SubwordTextEncoder.load_from_file('Seq2Seq/tokenizer_src')\n",
    "# tokenizer_trg = tfds.features.text.SubwordTextEncoder.load_from_file('Seq2Seqtokenizer_trg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521, 2631, 1515, 276, 1, 3093, 33, 70, 6, 1979, 1, 8, 70, 130, 4, 130, 70, 5, 213, 854, 110, 2800, 3822]\n",
      "Wie du weisst, sage ich immer zu mir, ist immer alles und alles immer in deinem Kopf.\n"
     ]
    }
   ],
   "source": [
    "''' Test tokenizer. '''\n",
    "\n",
    "sample_string = 'Wie du weisst, sage ich immer zu mir, ist immer alles und alles immer in deinem Kopf.'\n",
    "\n",
    "encoded_string = tokenizer_src.encode(sample_string)\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = tokenizer_src.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  521 ----> Wie \n",
      " 2631 ----> du \n",
      " 1515 ----> weis\n",
      "  276 ----> st\n",
      "    1 ----> , \n",
      " 3093 ----> sage \n",
      "   33 ----> ich \n",
      "   70 ----> immer \n",
      "    6 ----> zu \n",
      " 1979 ----> mir\n",
      "    1 ----> , \n",
      "    8 ----> ist \n",
      "   70 ----> immer \n",
      "  130 ----> alles \n",
      "    4 ----> und \n",
      "  130 ----> alles \n",
      "   70 ----> immer \n",
      "    5 ----> in \n",
      "  213 ----> de\n",
      "  854 ----> ine\n",
      "  110 ----> m \n",
      " 2800 ----> Kopf\n",
      " 3822 ----> .\n"
     ]
    }
   ],
   "source": [
    "''' Print subwords. '''\n",
    "\n",
    "for ts in encoded_string:\n",
    "    print('{:5} ----> {}'.format(ts, tokenizer_src.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end token to input and target\n",
    "def encode(src, trg):\n",
    "    src = [tokenizer_src.vocab_size] + tokenizer_src.encode(\n",
    "      src.numpy()) + [tokenizer_src.vocab_size+1]\n",
    "\n",
    "    trg = [tokenizer_trg.vocab_size] + tokenizer_trg.encode(\n",
    "      trg.numpy()) + [tokenizer_trg.vocab_size+1]\n",
    "  \n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap encoding in a tf.py_function to access it through map\n",
    "def tf_encode(src, trg):\n",
    "    result_src, result_trg = tf.py_function(encode, [src, trg], [tf.int64, tf.int64])\n",
    "    result_src.set_shape([None])\n",
    "    result_trg.set_shape([None])\n",
    "\n",
    "    return result_src, result_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to samples with MAX_LENGTH tokens.\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "      return tf.logical_and(tf.size(x) <= max_length,\n",
    "                            tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 67), dtype=int64, numpy=\n",
       " array([[4032,  389, 1457, ...,    0,    0,    0],\n",
       "        [4032,  521,   26, ...,    0,    0,    0],\n",
       "        [4032,  113, 2721, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [4032,   28, 1245, ...,    0,    0,    0],\n",
       "        [4032, 3692, 3808, ...,    0,    0,    0],\n",
       "        [4032,   79,    3, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 125), dtype=int64, numpy=\n",
       " array([[4033,  388,  505, ...,    0,    0,    0],\n",
       "        [4033, 2873,    8, ...,    0,    0,    0],\n",
       "        [4033,  360,   48, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [4033, 1209,   96, ...,    0,    0,    0],\n",
       "        [4033, 3749, 2352, ...,    0,    0,    0],\n",
       "        [4033, 1042,    3, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_batch, trg_batch = next(iter(val_dataset))\n",
    "src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "[Notebook about positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This look-ahead mask masks future tokens in a sequence.'''\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "    to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "        \n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point wise feed forward network\n",
    "\n",
    "This network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Consists of:\n",
    "\n",
    "1. Input Embedding\n",
    "2. Positional Encoding\n",
    "3. N encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        \n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Consists of:\n",
    "\n",
    "1. Output Embedding\n",
    "2. Positional Embedding\n",
    "3. N decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "    \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "            \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "The Transformer consists of encoder, decoder and a final linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "    \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers=2\n",
    "d_model=128\n",
    "dff=256\n",
    "num_heads=4\n",
    "EPOCHS=150\n",
    "\n",
    "input_vocab_size = tokenizer_src.vocab_size + 2\n",
    "target_vocab_size = tokenizer_trg.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Adam with a custom learning rate scheduler (from the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For training: remove old checkpoints. '''\n",
    "!rm -r 'Seq2Seq/checkpoints/' # for training: remove existing checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"Seq2Seq/checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-1\n",
      "Epoch 50 Loss 4.0500 Accuracy 0.1966\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-2\n",
      "Epoch 100 Loss 0.1977 Accuracy 0.5755\n",
      "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-3\n",
      "Epoch 150 Loss 0.0432 Accuracy 0.5848\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # inp -> src, tar -> trg\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        \n",
    "            \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "        \n",
    "        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                             train_loss.result(), \n",
    "                                                             train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_src.vocab_size]\n",
    "    end_token = [tokenizer_src.vocab_size + 1]\n",
    "    \n",
    "    inp_sentence = start_token + tokenizer_src.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "    \n",
    "    decoder_input = [tokenizer_trg.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "        \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "        \n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_trg.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "            \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "            \n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, target='-', plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "    predicted_sentence = tokenizer_trg.decode([i for i in result if i < tokenizer_trg.vocab_size])  \n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ob sie uns lachen oder weinen macht, wir belachen oder beweinen Abschaffenswertes.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Kunst lebt von den Fehlern der Welt.\"\n",
    "translate(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walking through latent space (word embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[309, 640, 41, 18, 3581, 1388, 3180, 31, 358, 2834, 3822]\n"
     ]
    }
   ],
   "source": [
    "# ''' Encode a sentence to retrieve ids. '''\n",
    "print(tokenizer_src.encode('Aufwertung des Selbstmodells durch dessen Formalisierung.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formalisierung\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_src.decode([2834]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_id = 2834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrieve learned embeddings. '''\n",
    "# it is a matrix of shape (1, vocab_size, embedding-dimension)\n",
    "e = transformer.encoder.embedding\n",
    "weights = e.get_weights().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04585315 -0.05057897  0.0362612   0.04257246 -0.07136862  0.05591803\n",
      "  0.03278057 -0.06143778  0.00618251 -0.03838155 -0.10599931  0.10329395\n",
      " -0.05787181  0.08018018  0.00537649  0.08309967  0.106056    0.07382646\n",
      " -0.0269549  -0.05182509 -0.03455293 -0.00505163 -0.00166758  0.02991542\n",
      "  0.02471358 -0.06029985 -0.04304826  0.06371844  0.01733958  0.0279441\n",
      " -0.02159692 -0.08514401 -0.05884919  0.03114592  0.07437395 -0.04945388\n",
      " -0.10289568  0.0309912  -0.0358796   0.10355988 -0.00079994 -0.02894607\n",
      " -0.04770349 -0.07436524 -0.02258739  0.01571286  0.10603911 -0.09613569\n",
      " -0.13266215 -0.10592959 -0.0160829  -0.01319074  0.0384906  -0.0509856\n",
      "  0.01403771  0.01202052  0.02398745  0.00178476  0.05008786 -0.07558156\n",
      " -0.02897525  0.00308615 -0.01544227  0.0696001  -0.00254706  0.0116569\n",
      "  0.0878296   0.02139591 -0.01359293  0.06344606  0.02299168 -0.05707199\n",
      "  0.04110349  0.05555265  0.05628436 -0.02213254  0.02659026 -0.01314045\n",
      " -0.03082482 -0.05694765 -0.01538857  0.05024926 -0.0216227   0.10276271\n",
      "  0.06275526 -0.09168332  0.04855673 -0.10914605  0.02181958  0.09606881\n",
      "  0.00462361 -0.05614655  0.0370901  -0.06716689  0.05095119  0.13220666\n",
      "  0.01151595  0.01935695  0.01254304 -0.02139967  0.03028584 -0.0668594\n",
      "  0.07245634  0.03851087 -0.0290256   0.10461431  0.02079896 -0.08735697\n",
      "  0.01752204 -0.03954041 -0.08043348 -0.05433587  0.03046023 -0.03014525\n",
      "  0.02082607  0.13892177  0.05605099 -0.05722263 -0.02244718 -0.04959518\n",
      "  0.02759586 -0.02941875  0.02408498  0.00282053  0.01462301  0.05086414\n",
      " -0.03430207  0.0667028 ]\n"
     ]
    }
   ],
   "source": [
    "''' Weights of tok_id. '''\n",
    "weights_backup = weights[0][tok_id].copy()\n",
    "# used as backup for restoring\n",
    "print(weights_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Restore weights. '''\n",
    "# weights[0][tok_id] = weights_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Iterate through the vector. Set each time one value of the vector to 0. '''\n",
    "translations = []\n",
    "for i in range(128):\n",
    "    # restore weights\n",
    "    weights[0][tok_id] = weights_backup\n",
    "    # change i\n",
    "    weights[0][tok_id][i] = 0.0\n",
    "    # set weights\n",
    "    e.set_weights(weights)\n",
    "    translations.append(translate('Aufwertung des Selbstmodells durch dessen Formalisierung.'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Reduce to unique sentences. '''\n",
    "translations_set = set(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es blieb jedoch nicht mehr nur bei der Vorstellung ermöglicht eine Maschine, die Neverarbeitung von sich selbst und Maschine, sondern eine Maschine.\n",
      "Es findet eine Nervennetzen, sich nicht mehr repräsentiert sich präferiert und ihm bewusst von sich nicht fest, sondern eine Maschine.\n",
      "Es blieb jedoch nicht mehr nur bei der Vorstellung ermöglicht eine Maschine, löst eine Spiegel diese selbst hat, sondern eine Maschine.\n",
      "Es blieb jedoch nicht mehr nur bei der Vorstellung ermöglicht eine Maschine, die Newtopologische Kunst, braucht, sondern eine Spiegel keines zur Kunst ist.\n",
      "Es gibt es nur eine Person von unserem Gehirn kontinuierlichen Spektrums an von sich selbst produziert.\n",
      "Es blieb jedoch nicht mehr nur bei der Vorstellung ermöglicht eine Maschine, löst eine Spiegel diese selbst hat, d.\n",
      "Es gibt es nur eine Person von sich selbst zuteilen, d.\n",
      "Es gibt es nur ein Werk nur zusammen mit der Kunst oder absprechen.\n",
      "Es findet eine Nervennetzen, sich nicht direkt mit einer Person von sich selbst versuchen, was ein Seziertisch aus, kann kann.\n",
      "Es findet eine Nervennetzen, sich nicht direkt mit einer Person von sich selbst hat, dbarkeit des sich selbst Nutzen uns nicht exekuting erfasst werden.\n",
      "Es findet eine Nervennetzen, sich nicht direkt mit einer Person von sich selbst versuchen, was ein Seziertisch aus, braucht.\n",
      "Es blieb jedoch nicht mehr nur eine Maschine, die entweder stimmt, selbst und die selbst oder im Subjekt.\n",
      "Es findet eine Nervennetzen, sich nicht mehr repräsentiert sich präferiert und ihm bewusst von mir entsprechende sich nicht ex [Abstraktionen.\n"
     ]
    }
   ],
   "source": [
    "for t in translations_set:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
