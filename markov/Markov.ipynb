{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 52186\n",
      "['Paul', 'Valérys', 'Form', ':', 'Sie', 'wird', 'gespeist', 'von', 'seinem', 'unermüdlichen', 'Drang', 'zum', 'Objektivieren', 'und', ',', 'mit', 'Cézannes', 'Wort', ',', 'Realisieren']\n"
     ]
    }
   ],
   "source": [
    "''' Read text and tokenize it with NLTK. '''\n",
    "from nltk.tokenize import word_tokenize as tok\n",
    "text = open('../dataset.txt', 'r').read()\n",
    "token = tok(text)\n",
    "print('Number of tokens:',len(token))\n",
    "print(token[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Store for each word in the corpus all n next tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a generator with pairs of tokens. '''\n",
    "def make_pairs(token, n_token=1):\n",
    "    for i in range(len(token)-n_token):\n",
    "        yield (token[i], token[i+1:i+1+n_token])\n",
    "\n",
    "pairs = make_pairs(token, n_token=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Paul', ['Valérys'])\n",
      "('Valérys', ['Form'])\n",
      "('Form', [':'])\n"
     ]
    }
   ],
   "source": [
    "''' Display first pairs. \n",
    "If executed, make_pairs() should be executed again. '''\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(next(iter(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8766\n"
     ]
    }
   ],
   "source": [
    "''' Create a vocabulary of all tokens and map them to their next tokens. '''\n",
    "vocabulary = {}\n",
    "for current_token, next_token in pairs:\n",
    "    if current_token in vocabulary.keys():\n",
    "        vocabulary[current_token].append(' '.join(next_token))\n",
    "    else:\n",
    "        vocabulary[current_token] = [' '.join(next_token)]\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', ',', 'ergebe', 'ist']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Display all options for »Selbstgespräch«. '''\n",
    "vocabulary['Selbstgespräch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "\n",
    "Get all options for one token from the vocabulary and pick one of them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function can handle a vocabulary with {word: multiple words} pairs.\n",
    "If lookup=True it prints all options for each input token. '''\n",
    "\n",
    "def generate_text(input_, n_token=12, lookup=False):\n",
    "    from nltk.tokenize import word_tokenize as tok\n",
    "    import string\n",
    "    import numpy as np\n",
    "    # tokenize \n",
    "    gentext = tok(input_)\n",
    "    gentext_lookup = tok(input_) # this string stores all options\n",
    "\n",
    "    for i in range(n_token):\n",
    "        # join array to string (necessary for n > 1)\n",
    "        gentext = ' '.join(gentext)\n",
    "        # split it again to get access to the last token\n",
    "        gentext = tok(gentext)\n",
    "        # get all possible following tokens\n",
    "        options = vocabulary[gentext[-1]]\n",
    "        # choose one of them\n",
    "        choice = np.random.choice(options)\n",
    "        gentext.append(choice)\n",
    "        # insert all possible values if lookup is true\n",
    "        if lookup:\n",
    "            # append all options\n",
    "            options = ' | '.join(options)\n",
    "            options = ': {  ' + options + '}\\n\\n'\n",
    "            gentext_lookup.append(options)\n",
    "            gentext_lookup.append(choice)\n",
    "            \n",
    "    # transform generated text to string with correct whitespaces\n",
    "    out = ''\n",
    "    for token in gentext:\n",
    "        if token in string.punctuation:\n",
    "            out += token\n",
    "        else:\n",
    "            out += ' ' + token\n",
    "    if lookup:\n",
    "        out += \"\\n\\n\"\n",
    "        out += ''.join(gentext_lookup)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Das Selbstgespräch, dass nicht mehr da sie über den hier nichts notiert,\n",
      " Das Selbstgespräch ist in ein typisches neuronales Verschaltungsmuster repräsentiert außer Zahlen, sie sich\n",
      " Das Selbstgespräch, immer dasselbe immer und Produktionsverhältnisse erzeugen neue Maschine, welcher einem\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_text('Das Selbstgespräch', 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "This method picks the next word not just based on one token, but on n token. Typical n-grams are of length 2 (bigrams) or 3 (trigrams).\n",
    "For a small dataset trigrams may be too long, because they reduce the number of choices for each string of n words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a generator with pairs of tokens. '''\n",
    "def make_n_gram_pairs(token, n_grams=2):\n",
    "    for i in range(len(token)-n_grams-1):\n",
    "        yield (' '.join(token[i:i+n_grams]), token[i+n_grams])\n",
    "\n",
    "pairs = make_n_gram_pairs(token, n_grams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Paul Valérys', 'Form')\n",
      "('Valérys Form', ':')\n",
      "('Form :', 'Sie')\n"
     ]
    }
   ],
   "source": [
    "''' Display first pairs. '''\n",
    "for i in range(3):\n",
    "    print(next(iter(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33850\n"
     ]
    }
   ],
   "source": [
    "''' Create a vocabulary of all n-token-strings and map them to their next token. '''\n",
    "vocabulary = {}\n",
    "\n",
    "for current_token, next_token in pairs:\n",
    "    if current_token in vocabulary.keys():\n",
    "        vocabulary[current_token].append(next_token)\n",
    "    else:\n",
    "        vocabulary[current_token] = [next_token]\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ist']\n"
     ]
    }
   ],
   "source": [
    "''' Display all options for »Das Selbstgespräch«. '''\n",
    "print(vocabulary['Das Selbstgespräch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function requires an input of at least n_grams token. '''\n",
    "def generate_text_n_grams(input_, n_token=12, n_grams=1):\n",
    "    from nltk.tokenize import word_tokenize as tok\n",
    "    import numpy as np\n",
    "    import string\n",
    "    \n",
    "    # tokenize input\n",
    "    gentext = tok(input_)\n",
    "    try:\n",
    "        options = vocabulary[' '.join(gentext[-n_grams:])]\n",
    "    except:\n",
    "        return 'No key available for: ' + ' '.join(gentext[-n_grams:])\n",
    "    for i in range(n_token):\n",
    "        # get all options for the last n_grams of gentext and choose one\n",
    "        options = vocabulary[' '.join(gentext[-n_grams:])]\n",
    "        choice = np.random.choice(options)\n",
    "        # append it to the generated text\n",
    "        gentext.append(choice)\n",
    "    output = ''\n",
    "    for token in gentext:\n",
    "        if token in string.punctuation:\n",
    "            output += token\n",
    "        else:\n",
    "            # add a whitespace if token is not a punctuation\n",
    "            output += ' ' + token\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Das Selbstgespräch ist ein Gegenstand mit ästhetischem Anspruch. Dieser neuronale Prozess benötigt auf\n",
      " Das Selbstgespräch ist der Materialismus, der beide verwandelt: Das Selbst ist durch\n",
      " Das Selbstgespräch ist ein erster und starker Hinweis dafür, dass nicht nur zu\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_text_n_grams(\"Das Selbstgespräch\", n_token=12, n_grams=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "https://mb-14.github.io/tech/2018/10/24/gomarkov.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
